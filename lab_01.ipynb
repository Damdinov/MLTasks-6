{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковое моделирование\n",
    "В данном задании вы реализуете:\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- EM-algorithm\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы примените это к:\n",
    "- Language recognition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подпись**: *Дамдинов Ринчин Гармаевич, ФИТ, 1-й курс магистратуры*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Возникает закономерный вопрос, а зачем нам это надо? Например, в задачах _машинного перевода_ частенько нужно среди нескольких предложений выбирать наиболее вероятный перевод (который является естественным для человеческого глаза). Также это чрезвычайно полезно в задаче _исправления опечаток_ и _распозновании речи_.\n",
    "\n",
    "Наша задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **язковой моделью** (LM от Language Model).\n",
    "\n",
    "Пришло время вспомнить _цепное правило_ (chain rule) $P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1})$. Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения ситуации применим **марковское предположение**: $P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$ для некоторого фиксированного (небольшого) $k$. Это предположение интуитивно ясно и говорит нам о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пришло время написать класс для хранения n-грам. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "    \n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "            \n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {i: Counter() for i in xrange(self.__max_n + 1)}\n",
    "\n",
    "        __max = 0        \n",
    "        ### YOUR CODE HERE   \n",
    "        number_of_words = 0\n",
    "        self.add_unk_token()\n",
    "        \n",
    "        for sent in sents:\n",
    "            if( self.__max_n > len(sent)):\n",
    "                self.__max = len(sent)\n",
    "            else:\n",
    "                self.__max = self.__max_n\n",
    "\n",
    "            for indent in xrange(1, self.__max + 1):\n",
    "                for begin in xrange(len(sent) + 1 - indent):\n",
    "                    self.__ngrams[indent][tuple(sent[begin:begin + indent])] += 1\n",
    "\n",
    "        self.__ngrams[0][()] = len(self.__ngrams[1])\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or u'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][(u'UNK',)] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n.\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            print(len(ngram))\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][(u'UNK', )]\n",
    "        return self.__ngrams[len(ngram)][ngram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте скачаем корпус и запустим на нем наши эксперименты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338\n",
      "3282\n",
      "29\n",
      "1\n",
      "2779575\n"
     ]
    }
   ],
   "source": [
    "storage = NGramStorage(train_sents, 3)\n",
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('Muammar',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\rm PP}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "кстати, из этой формулы видно, что задача по минимизации перплексии равносильна задаче по максимизации вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию по подсчету перплексии (сначала посмотрите как реализован StraightforwardProbabilityEstimator, а далее уже пишите код)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    ### YOUR CODE HEREzz\n",
    "    # Avoid log(0) by replacing zero by 10 ** (-50).\n",
    "    N = 0\n",
    "    logSum = 0;\n",
    "    for sent in sents:\n",
    "        N += len(sent)\n",
    "    logSum = np.sum([math.log(estimator.prob(sent) + 10 ** (-50)) for sent in sents])\n",
    "    perp = math.exp( -1./N * logSum)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. Что-ж, пора это реализовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "\n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not (isinstance(word, str) or isinstance(word, unicode)):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in xrange(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 238.19490166\n",
      "0.000376909343384\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 38.4472732112\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "17\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(storage(tuple('To'.split())))\n",
    "print(storage(tuple('To be'.split())))\n",
    "print(storage(tuple('To be or'.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "* Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?\n",
    "* Почему перплексия униграмной модели меньше, чем триграмной?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОТВЕТ:\n",
    "* Как минимум один ноль дает 'To be or', и UKN токен добавляется только для 1-грам\n",
    "* Перплексия возрастает с уменьшением вероятности, а вероятность уменьшается с увеличением числа слов в кортеже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Каким-то образом мы хотим избавиться от нулевых вероятностей. Наиболее простой алгоритм борьбы с этой проблемой следующий: для оценки $P(w_{N} \\mid w_1^{N - 1})$ мы будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пришло время снова пописать код. Реализуйте класс, симулирующий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not (isinstance(word, str) or isinstance(word, unicode)):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        ### YOUR CODE HERE\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        pcounts = self.__storage(context + (word, )) + self.__delta\n",
    "        ccounts = self.__storage(context) + self.__delta * self.__storage[0][()]\n",
    "\n",
    "        prob = 1. * pcounts / ccounts\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in xrange(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best delta: 0.0001983772739\n"
     ]
    }
   ],
   "source": [
    "avg=0.\n",
    "for i in range(1, storage .max_n):\n",
    "    avg+=sum(storage[i].values()) / len(storage[i]) \n",
    "avg = avg / (storage.max_n-1)\n",
    "best_delta = avg/storage[0][()]\n",
    "print \"best delta:\", best_delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 144.551659\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to find out best delta parametr. We will not provide you any strater code.\n",
    "### YOUR CODE HERE\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "best_delta = 1\n",
    "\n",
    "def rosen(x):\n",
    "    return perplexity( LaplaceProbabilityEstimator(storage, best_delta) , test_sents)\n",
    "\n",
    "res = minimize(rosen, best_delta, method='nelder-mead', options={'xtol': 1e-8, 'disp': True, 'maxiter': 50})\n",
    "### END YOUR CODE\n",
    "best_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace estimator perplexity = 124.282988583\n",
      "0.000366017304912\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **тупого откатывания** невероятно проста (на то оно и тупое!). Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, тогда будем использовать $k$-грамы. Если эта вероятность крайне мала, то будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не принципиально важно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, симулирующий сглаживание тупым откатыванием. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        prob = 0\n",
    "        while(prob == 0):\n",
    "            prob = self.__base_estimator(word, context)\n",
    "            context = context[1:]\n",
    "        prob *= self.__mult\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate SCORE of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in xrange(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 64.4132411335\n",
      "5.3865652725e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(sbackoff_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "* Почему бессмысленно измерять перплексию в случае **Stupid backoff**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В SB не получается вероятносное распределение, а для перплексии оно нужно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Ну а далее просто постулируем, что\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$. Казалось бы, их нужно несколько, ибо наша модель должна уметь считать вероятности $P(w_3 \\mid w_1, w_2)$, а иногда $P(w_2 \\mid w_1)$. Если мы тупо обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        if context == ():\n",
    "            return self.__base_estimator(word, context)\n",
    "        \n",
    "        context = self.__base_estimator.cut_context(context)     \n",
    "        lambdas = np.zeros(len(context))\n",
    "                \n",
    "        extraLambda = np.sum([ self.lambdas[i] for i in xrange(len(context) + 1,len(self.lambdas))]) / len(context)\n",
    "        prob = np.sum([(self.lambdas[-i] + extraLambda)* self.__base_estimator(word, context[i:]) \n",
    "                       for i in range(len(context) + 1)])\n",
    "        ### END YOUR CODE\n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in xrange(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 50.4964003997\n",
      "0.00110501245471\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "print(interpol_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается один вопрос: как подбирать $\\bar\\lambda$? Для этого обычно применяется EM-алгоритм. Сразу опишем лишь готовую формулу. Пусть $\\bar\\lambda^t$ соответствует набору коэффициентов на $t$-м шаге. $\\bar\\lambda^0$ задаем произвольно.\n",
    "\n",
    "* **E-step**:\n",
    "$$\n",
    "    \\hat\\lambda_j^t = \\sum_{w_1^N} \\frac{\\lambda_j^t \\cdot \\hat P_{S}(w_N \\mid w_{N-j+1}^{N-1})}{\\sum_{i=1}^N \\lambda_i^t \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1})}.\n",
    "$$\n",
    "* **M-step**:\n",
    "$$\n",
    "    \\lambda_j^{t+1} = \\frac{\\hat\\lambda_j^t}{\\sum_{i=1}^N \\hat\\lambda_i^t}.\n",
    "$$\n",
    "\n",
    "Останавливаться алгоритм должен, когда $\\|\\hat\\lambda_j^t - \\hat\\lambda_i^{t+1}\\| < \\varepsilon$.\n",
    "\n",
    "Формулы выписаны, то бишь самое сложное сделано. Вам остается это лишь реализовать. Документацию для таких формул можете потренироваться написать сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E_step(test_storage, s_estimator, i_estimator):\n",
    "    ### YOUR CODE HERE\n",
    "    N = len(i_estimator.lambdas)\n",
    "    estimated_lambdas = np.zeros(len(i_estimator.lambdas)) \n",
    "    \n",
    "    for i in xrange( 1, test_storage.max_n+1):\n",
    "        for tlp in list(test_storage[i]):\n",
    "            denominator = i_estimator(tlp[len(tlp)-1],tlp[:len(tlp)-1])\n",
    "            if( denominator != 0):\n",
    "                for j in xrange(len(estimated_lambdas)):\n",
    "                    numerator = s_estimator(tlp[len(tlp)-1], tlp[ len(tlp)-j : len(tlp)-1 ] )\n",
    "                    estimated_lambdas[j] += i_estimator.lambdas[j] * numerator / denominator\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return estimated_lambdas\n",
    "\n",
    "def M_step(estimated_lambdas):\n",
    "    ### YOUR CODE HERE\n",
    "    new_lambdas = estimated_lambdas / np.sum(estimated_lambdas)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return new_lambdas\n",
    "\n",
    "def EM_algorithm(test_storage, s_estimator, i_estimator, epsilon=0.03):\n",
    "    ### YOUR CODE HERE\n",
    "    old_lambdas = i_estimator.lambdas \n",
    "    while True:\n",
    "        new_lambdas = M_step(E_step(test_storage, s_estimator, i_estimator))\n",
    "        i_estimator.lambdas = new_lambdas\n",
    "        if np.linalg.norm(new_lambdas - old_lambdas) < epsilon:\n",
    "            break\n",
    "        old_lambdas = new_lambdas#estimated_lambdas\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return i_estimator.lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate train into train and test\n",
    "train_set = train_sents[:int(0.6 * len(train_sents))]\n",
    "test_set = train_sents[int(0.6 * len(train_sents)):]\n",
    "MAX_N = 3\n",
    "train_storage = NGramStorage(train_set, MAX_N)\n",
    "test_storage = NGramStorage(test_set, MAX_N)\n",
    "s_estimator = StraightforwardProbabilityEstimator(train_storage)\n",
    "\n",
    "# Make starting assumption\n",
    "starting_lambdas = np.array([0.33, 0.33, 0.34])\n",
    "i_estimator = InterpolationProbabilityEstimator(s_estimator, starting_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It can take some time\n",
    "best_lambdas = EM_algorithm(test_storage, s_estimator, i_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49792053,  0.49792053,  0.00415894])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 47.3973683768\n",
      "0.00017471961809\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "i_estimator = InterpolationProbabilityEstimator(simple_estimator, best_lambdas)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(i_estimator, test_sents)))\n",
    "print(i_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые исползуются в паре-тройке контекстов получают маленькие вероятности. Авторы данного сглаживания формализовали это следующим образом. Введем обозначения\n",
    "\n",
    "$$\n",
    "    N_{1+}(\\cdot w_2) := \\left|\\{w_1 : c(w_1, w_2) > 0\\}\\right|,\n",
    "$$\n",
    "$$\n",
    "    N_{1+}(\\cdot \\cdot) := \\sum_{w_2} N_{1+}(\\cdot w_2),\n",
    "$$\n",
    "$$\n",
    "    \\hat P_{KN} (w_1) = \\frac{N_{1+}(\\cdot w_1)}{N_{1+}(\\cdot \\cdot)}.\n",
    "$$\n",
    "\n",
    "Далее мы используем реккурентное соотношение\n",
    "\n",
    "$$\n",
    "    \\hat P_{KN}(w_{N} \\mid w_1^{N-1}) = \\frac{{\\rm max}\\{c(w_1^N) - \\delta, 0\\}}{\\sum_{w_N}c(w_1^{N-1}w_N)} + \\frac{\\delta}{\\sum_{w_N}c(w_1^{N-1}w_N)}N_{1+}(w_1^{N-1}\\cdot)\\hat P_{KN}(w_N \\mid w_2^{N-1}).\n",
    "$$\n",
    "\n",
    "Для вас дело за малым — реализовать это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        self.__postfixs = Counter()\n",
    "        self.__prefixs = Counter()\n",
    "\n",
    "        # подсчет числа появлений wi после любого др. слова\n",
    "        # N+(*w)\n",
    "        if(self.__storage.max_n > 1):\n",
    "            for i in self.__storage[2]:\n",
    "                self.__postfixs[i[-1]] += 1\n",
    "        # N+(w*)\n",
    "        for i in range(1, self.__storage.max_n):\n",
    "            for j in self.__storage[i + 1]:\n",
    "                self.__prefixs[j[:-1]] += 1\n",
    "\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not (isinstance(word, str) or isinstance(word, unicode)):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #Т.е. если 1-грамма \n",
    "        if context == ():\n",
    "            prob = max([self.__postfixs[word] - self.__delta, 0]) / len(storage[2]) + self.__delta / self.__storage[0][()]\n",
    "        #если частота контекста 0, берем меньший контекст\n",
    "        elif (self.__storage(context) == 0):\n",
    "            prob = self(word, context[1:])\n",
    "        else:\n",
    "            prob = self.__prefixs[context] * self.__delta * self(word, context[1:])\n",
    "            prob += np.max([self.__storage(context + (word, )) - self.__delta, 0])\n",
    "            prob /= self.__storage(context) \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in xrange(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 150.975203892\n",
      "4.29271574498e-06\n",
      "4.7454835077e-13\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(kn_estimator.prob('To be'.split()))\n",
    "print(kn_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение языка документа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частотности символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папочке _data_ находятся две папочки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папочек _full_ и _plain_ находятся папочки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папочке находится файл _ans.csv_, в котором вы можете найти правильные ответ и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import csv\n",
    "\n",
    "trainFiles = os.path.join(os.getcwd(), 'plain', 'train')\n",
    "testFiles = os.path.join(os.getcwd(), 'plain', 'test')\n",
    "plain = os.path.join(os.getcwd(), 'plain')\n",
    "\n",
    "answers = {}\n",
    "with open(os.path.join(plain, 'ans.csv'), 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for rows in reader:\n",
    "        answers[rows[0]] = rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LClf():\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.__dict = {}\n",
    "        self.__n = n\n",
    "        \n",
    "    def fit(self, fileNames, train_path):\n",
    "        for filename in fileNames:\n",
    "            with open(os.path.join(train_path, filename), 'r') as csvfile:\n",
    "                train_sents = csvfile.readlines()\n",
    "                train = [ [letter for letter in word] for sent in train_sents for word in sent.split(' ')]\n",
    "                \n",
    "                storage = NGramStorage(train, self.__n)\n",
    "                simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "                sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "                #StupidBackoffProbabilityEstimator(NGramStorage(train, self.__n),.4)\n",
    "                self.__dict[filename.split('.')[0]] = sbackoff_estimator\n",
    "         \n",
    "    def predict(self, fileNames, test_path):\n",
    "        res = {}\n",
    "        for filename in fileNames:\n",
    "            with open(os.path.join(test_path, filename), 'r') as csvfile:\n",
    "                test_sents = csvfile.readlines()\n",
    "                test = [ [letter for letter in word] for sent in test_sents for word in sent.split(' ')]\n",
    "                \n",
    "                #res[filename.split('.')[0]] = min(self.__dict.keys(), key = lambda cl: -log(self.__dict[cl]) + \\\n",
    "                #    sum(-log(prob.get((cl,feat), 10**(-7))) for feat in feats))\n",
    "                \n",
    "                res[filename.split('.')[0]] = min(self.__dict.iterkeys(), \n",
    "                                key=(lambda key: perplexity(self.__dict[key], test)))\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles(path):\n",
    "    test_texts = []\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        test_texts.extend(filenames)\n",
    "        break\n",
    "    return test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LClf(3)\n",
    "clf.fit(getFiles(trainFiles),trainFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(getFiles(testFiles), testFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На основе языковых моделей, на 3-граммах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resb = np.sum([ y_pred[key] == answers[key] for key in answers.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (float(resb) / len(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " right 240\n",
      " from 240\n"
     ]
    }
   ],
   "source": [
    "print' right %i' % (resb)\n",
    "print' from %i' % (len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LClf(1)\n",
    "clf.fit(getFiles(trainFiles),trainFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(getFiles(testFiles), testFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Наивный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      " right 216\n",
      " from 240\n"
     ]
    }
   ],
   "source": [
    "resb = np.sum([ y_pred[key] == answers[key] for key in answers.keys()])\n",
    "print (float(resb) / len(answers))\n",
    "print' right %i' % (resb)\n",
    "print' from %i' % (len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
